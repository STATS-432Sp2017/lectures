---
title: "Chapter 1"
author: "DJM"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  slidy_presentation:
    height: 1080
    width: 1920
    css: "http://mypage.iu.edu/~dajmcdon/teaching/DJMslidy.css"
---

## Simplifying variance

$$
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\given}{\ \vert\ }
$$


> Why does $\Expect{(Y-m)^2} = \Expect{Y-m}^2 + \Expect{Y-m}$?
(note the typo in eq. 1.3)

Recall the usual variance formula:
\[
\Var{X} = \Expect{X^2} - \Expect{X}^2
\]

This formula is very useful 

[It gets used again for, e.g., (1.13)--(1.14) and (1.29)--(1.30).]

## Causation, independence and regression

For any two variables $Y$ and $X$, we can write
\[
Y \given X = \mu(X) + \eta(X)
\]
such that $\Expect{\eta(X)}=0$.

* Suppose, $\mu(X)=\mu_0$ (constant in $X$), are $Y$ and $X$ independent?
* Suppose $Y$ and $X$ are independent, is $\mu(X)=\mu_0$?

## The regression function


> What is the notation $\mu(x)$?

* In mathematics: $\mu(x) = \Expect{Y \given X=x}$.

* In words: Regression is really about estimating the mean. 
  1. If $Y\sim \textrm{N}(\mu, 1)$, our best guess for a __new__ $Y$ is $\mu$
  2. For regression, we let the mean $(\mu)$ __depend__ on $X$.
  3. Think of $Y\sim \textrm{N}(\mu(X), 1)$, then conditional on $X=x$, our best guess for a __new__ $Y$ is $\mu(x)$ [whatever this function $\mu$ is]

## MSE of the estimated regression function


> Explain the proof of the bias variance decomposition in (1.22)--(1.27).


## Linear smoothers

> What is a linear smoother?

1. Suppose I observe $Y_1,\ldots,Y_n$.
2. A linear smoother is any __prediction function__ that's linear in $\mathbf{Y}$.
  * Linear functions of $\mathbf{Y}$ are simply premultiplications by a matrix, i.e. $\hat{\mathbf{Y}} = \mathbf{WY}$ for any matrix $\mathbf{W}$.
3. Examples:
  * $\overline{Y} = \frac{1}{n}\sum Y_i = \frac{1}{n}\begin{bmatrix} 1 & 1 & \cdots & 1 \end{bmatrix} \mathbf{Y}$
  * Given $X$, $\hat{\mathbf{Y}} = \mathbf{X(X^\top X)}^{-1}\mathbf{X^\top Y}$
  * You will see many other smoothers in this class

## kNN as a linear smoother

(We will see __smoothers__ in more detail in Ch. 4)

1. For kNN, consider a particular pair $(Y_i, X_i)$
2. Find the $k$ covariates $X_j$ which are closest to $X_i$
3. Predict $Y_i$ with the average of those $X_j$'s
4. This turns out to be a linear smoother
  * How would you specify $\mathbf{W}$?

## Kernels


> What are "kernels"?

(Again, more info in Ch. 4)

* There are two definitions of "kernels". We'll use only 1.
* Recall the pdf for the Normal density:
  \[
  f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{\frac{1}{2\sigma^2}(x-\mu)^2\right\}
  \]
* The part that depends on the data ($x$), is a kernel
* The kernel has a _center_ ($\mu$) and a _range_ ($\sigma$)

## Kernels (part 2)


* In general, any function which integrates, is non-negative, and symmetric is a kernel in the sense used in the book
* You can think of any (unnormalized) symmetric density function (uniform, normal, Cauchy, etc.)
* The way you use a kernel is take a weighted average of nearby data to make predictions
* The weight of $X_j$ is given by the height of the density centered at $X_i$
* Examples:
  - The __Gaussian__ kernel is $K(x-x_0) = e^{-(x-x_0)^2/2}$
  - The __Boxcar__ kernel is $K(x-x_0) = I(x-x_0<1)$

## Kernels (part 3)

* You don't need the normalizing constant
* To alter the __support__: take $(x-x_0)/h$ and $K(z) = K(z)/h$
* Now, the range of the density is determined by $h$
* You can interpret kNN as a particular kind of kernel
  - The range is determined by $k$
  - The center is determined by $X_i$
  
## In-sample MSE for linear smoothers


Start from:
\[
\begin{align}
e &= y-wy\\
\Expect{\frac{1}{n}\lVert e\rVert^2} &= \Var{e} + \Expect{e}^2
\end{align}
\]

## Other questions

__What didn't I answer?__

